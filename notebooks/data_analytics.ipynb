{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74b4c60",
   "metadata": {},
   "source": [
    "# Comparative Analysis of all Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b70bc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Imports & Data Set Loading](#imports--data-set-loading)\n",
    "2. [Detailed Analysis](#detailed-analysis)\n",
    "    1. [Size Visualization](#1-size-visualization)\n",
    "    2. [Data Types and Missing Values](#2-data-types-and-missing-values)\n",
    "    3. [Identify Target Variables](#3-identify-target-variables)\n",
    "    4. [Target Variable Analysis](#4-target-variable-analysis)\n",
    "    5. [Feature Type Analysis](#5-feature-type-analysis)\n",
    "    6. [Statistical Summary of Numerical Features](#6-statistical-summary-of-numerical-features)\n",
    "    7. [Correlation Analysis](#7-correlation-analysis)\n",
    "    8. [Data Quality Check](#8-data-quality-check)\n",
    "3. [Baseline Model Performance](#baseline-model-performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae44e",
   "metadata": {},
   "source": [
    "## Imports & Data Set Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_datasets = ['bank', 'credit', 'income']\n",
    "synthetic_datasets = ['train_A', 'train_B', 'train_C']\n",
    "dataset_folder = os.path.join(\"..\", \"data\")\n",
    "real_dataset_path = os.path.join(dataset_folder, \"real\")\n",
    "synthetic_dataset_path = os.path.join(dataset_folder, \"synthetic\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "# Load real datasets\n",
    "for name in real_datasets:\n",
    "\n",
    "    df = pd.read_csv(os.path.join(real_dataset_path, f'{name}.csv'))\n",
    "    datasets[name] = df\n",
    "\n",
    "# Load synthetic datasets  \n",
    "for name in synthetic_datasets:\n",
    "\n",
    "    df = pd.read_csv(os.path.join(synthetic_dataset_path, f'{name}.csv'))\n",
    "    datasets[name] = df\n",
    "\n",
    "print(f\"\\nTotal datasets loaded: {len(datasets)}\")\n",
    "\n",
    "# Quick Overview of All Datasets\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"-\" * 50)\n",
    "for name, df in datasets.items():\n",
    "    dataset_type = \"Real\" if name in real_datasets else \"Synthetic\"\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"{name:12} | {dataset_type:9} | {df.shape[0]:6,} rows × {df.shape[1]:2} cols | {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89bcc80",
   "metadata": {},
   "source": [
    "## Detailed Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb9fdb",
   "metadata": {},
   "source": [
    "#### 1. Size Visualization\n",
    "\n",
    "What sizes do the 6 available data sets have? How many features (columns) and samples (rows) do they each have?\n",
    "\n",
    "\n",
    "Which data sets have comparatively many/ few features or samples available?\n",
    "\n",
    "Results:\n",
    "- \"Credit\" has considerably less samples available than \"bank\" or \"income\" -> need to make good use of them\n",
    "- The synthetic data sets all have comparatively few samples but many features -> good feature extraction needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef9cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Samples per dataset\n",
    "dataset_names = list(datasets.keys())\n",
    "sample_counts = [datasets[name].shape[0] for name in dataset_names]\n",
    "colors = ['lightblue' if name in real_datasets else 'lightcoral' for name in dataset_names]\n",
    "\n",
    "ax1.bar(dataset_names, sample_counts, color=colors)\n",
    "ax1.set_title('Number of Samples per Dataset')\n",
    "ax1.set_ylabel('Sample Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Features per dataset\n",
    "feature_counts = [datasets[name].shape[1] for name in dataset_names]\n",
    "ax2.bar(dataset_names, feature_counts, color=colors)\n",
    "ax2.set_title('Number of Features per Dataset')\n",
    "ax2.set_ylabel('Feature Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='lightblue', label='Real'),\n",
    "                   Patch(facecolor='lightcoral', label='Synthetic')]\n",
    "ax2.legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ea5f1",
   "metadata": {},
   "source": [
    "#### 2. Data Types and Missing Values\n",
    "\n",
    "What data types do we have as features? Do we need to handle any missing values?\n",
    "\n",
    "Results:\n",
    "- None of the data sets contains missing values -> implementation of missing value handling optional\n",
    "- The real data sets contain a mixture of categorical (dtype('O'), also including the label column) and floating point features -> need to implement categorical feature encoding\n",
    "- The real data sets contain only floating point features and the integer label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4852db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{'='*20} {name.upper()} {'='*20}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_count = missing.sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Missing values: {missing_count} total\")\n",
    "        missing_cols = missing[missing > 0]\n",
    "        for col, count in missing_cols.items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  - {col}: {count} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"Missing values: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c09340",
   "metadata": {},
   "source": [
    "#### 3. Identify Target Variables\n",
    "\n",
    "How is the label column called in each data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d568a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify Target Variables\n",
    "\n",
    "print(\"Target Variable Detection:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "target_info = {}\n",
    "common_target_names = ['target', 'label', 'class', 'y', 'outcome', 'result']\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    target_col = None\n",
    "    \n",
    "    # Check for common target names\n",
    "    for candidate in common_target_names:\n",
    "        if candidate in df.columns.str.lower():\n",
    "            # Find the actual column name (case-insensitive)\n",
    "            actual_col = [col for col in df.columns if col.lower() == candidate][0]\n",
    "            target_col = actual_col\n",
    "            break\n",
    "    \n",
    "    # If not found, check last column if it looks like a target\n",
    "    if target_col is None:\n",
    "        last_col = df.columns[-1]\n",
    "        unique_vals = df[last_col].nunique()\n",
    "        if unique_vals <= 10:  # Likely categorical target\n",
    "            target_col = last_col\n",
    "            print(f\"{name:12} | Assumed target: '{target_col}' (last column, {unique_vals} unique values)\")\n",
    "        else:\n",
    "            print(f\"{name:12} | No clear target identified\")\n",
    "    else:\n",
    "        print(f\"{name:12} | Found target: '{target_col}'\")\n",
    "    \n",
    "    target_info[name] = target_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7424600",
   "metadata": {},
   "source": [
    "#### 4. Target Variable Analysis\n",
    "\n",
    "How many samples per class do we have in each data set?\n",
    "\n",
    "Results:\n",
    "- All data sets have binary labels\n",
    "- The real data sets (especially \"bank\" and \"target\") have a strong class imbalance -> need a strategy to handle this\n",
    "- The synthetic data sets have quite balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66be6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Target Variable Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (name, df) in enumerate(datasets.items()):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    target_col = target_info[name]\n",
    "    \n",
    "    if target_col and target_col in df.columns:\n",
    "        # Plot target distribution\n",
    "        value_counts = df[target_col].value_counts()\n",
    "        bars = ax.bar(range(len(value_counts)), value_counts.values, \n",
    "                     color='skyblue' if name in real_datasets else 'lightcoral')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, value_counts.values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{count}', ha='center', va='bottom')\n",
    "        \n",
    "        ax.set_title(f'{name.upper()}\\nTarget: {target_col}')\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_xticks(range(len(value_counts)))\n",
    "        ax.set_xticklabels(value_counts.index, rotation=0)\n",
    "        \n",
    "        # Calculate and show balance ratio\n",
    "        balance_ratio = value_counts.min() / value_counts.max()\n",
    "        ax.text(0.02, 0.98, f'Balance: {balance_ratio:.2f}', \n",
    "               transform=ax.transAxes, va='top', fontsize=10,\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        \n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No clear target\\nidentified', ha='center', va='center',\n",
    "               transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title(f'{name.upper()}\\nNo target found')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12a689",
   "metadata": {},
   "source": [
    "#### 5. Feature Type Analysis\n",
    "\n",
    "How many numerical (float) vs categorical features does each data set have? How large can we expect the value space for each categorical feature to be?\n",
    "\n",
    "Results:\n",
    "- The majority of features in the real data sets are categorical -> need to encode these somehow\n",
    "- Each categorical feature just has a very limited range of possible values -> compact encoding should be possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Type Breakdown:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    \n",
    "    # Numerical features\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_info[name] in numerical_cols:\n",
    "        numerical_cols.remove(target_info[name])\n",
    "    \n",
    "    # Categorical features  \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if target_info[name] in categorical_cols:\n",
    "        categorical_cols.remove(target_info[name])\n",
    "    \n",
    "    print(f\"  Numerical features: {len(numerical_cols)}\")\n",
    "    if len(numerical_cols) <= 5:\n",
    "        print(f\"    {numerical_cols}\")\n",
    "    \n",
    "    print(f\"  Categorical features: {len(categorical_cols)}\")\n",
    "    if len(categorical_cols) <= 5:\n",
    "        print(f\"    {categorical_cols}\")\n",
    "    \n",
    "    # Show cardinality for categorical features\n",
    "    if categorical_cols:\n",
    "        print(\"  Categorical feature cardinality:\")\n",
    "        for col in categorical_cols[:3]:  # Show first 3\n",
    "            cardinality = df[col].nunique()\n",
    "            print(f\"    {col}: {cardinality} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c3b23",
   "metadata": {},
   "source": [
    "#### 6. Statistical Summary of Numerical Features\n",
    "\n",
    "What are the means/ standard derivations/ mins/ maxs of the numerical features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in datasets.items():\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_info[name] in numerical_cols:\n",
    "        numerical_cols.remove(target_info[name])\n",
    "    \n",
    "    if len(numerical_cols) > 0:\n",
    "        print(f\"\\n{'='*15} {name.upper()} - NUMERICAL STATS {'='*15}\")\n",
    "        print(df[numerical_cols].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ea4cc",
   "metadata": {},
   "source": [
    "#### 7. Correlation Analysis\n",
    "\n",
    "For the numerical features: Do any of have correlating values?\n",
    "\n",
    "Result: Both, Train_B and Train_C show some kind of correlating features -> can ignore one of these features in the model, as this is probably duplicate information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcbd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrices = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if len(numerical_cols) > 1:\n",
    "        print(f\"\\n{'='*10} CORRELATION ANALYSIS: {name.upper()} {'='*10}\")\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df[numerical_cols].corr()\n",
    "        \n",
    "        # Find high correlations (excluding diagonal)\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:\n",
    "                    high_corr_pairs.append({\n",
    "                        'feature1': corr_matrix.columns[i],\n",
    "                        'feature2': corr_matrix.columns[j],\n",
    "                        'correlation': corr_val\n",
    "                    })\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            print(\"High correlations found (|r| > 0.7):\")\n",
    "            for pair in high_corr_pairs:\n",
    "                print(f\"  {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "        else:\n",
    "            print(\"No high correlations found (|r| > 0.7)\")\n",
    "        \n",
    "        # Collect correlation matrices for compact plotting\n",
    "        if len(numerical_cols) <= 10:\n",
    "            corr_matrices.append((name, corr_matrix))\n",
    "\n",
    "# Plot all correlation heatmaps in a single figure\n",
    "if corr_matrices:\n",
    "    n = len(corr_matrices)\n",
    "    ncols = 2\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(7 * ncols, 6 * nrows))\n",
    "    axes = np.array(axes).reshape(-1)  # Flatten in case of 1 row/col\n",
    "\n",
    "    for idx, (name, corr_matrix) in enumerate(corr_matrices):\n",
    "        ax = axes[idx]\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(\n",
    "            corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8}, ax=ax\n",
    "        )\n",
    "        ax.set_title(f'Correlation Matrix: {name.upper()}')\n",
    "    # Hide unused subplots\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be757e3",
   "metadata": {},
   "source": [
    "#### 8. Data Quality Check\n",
    "\n",
    "Are there any duplicate samples in the data sets?\n",
    "\n",
    "Result: Only \"income\" has duplicate samples -> should remove those before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Quality Assessment:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "quality_issues = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    issues = []\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        issues.append(f\"{duplicates} duplicate rows\")\n",
    "    \n",
    "    # Check for constant columns\n",
    "    constant_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        issues.append(f\"{len(constant_cols)} constant columns: {constant_cols}\")\n",
    "    \n",
    "    # Check for high-cardinality categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    high_card_cols = [col for col in categorical_cols if df[col].nunique() > 50]\n",
    "    if high_card_cols:\n",
    "        issues.append(f\"{len(high_card_cols)} high-cardinality categorical columns\")\n",
    "    \n",
    "    # Check missing values percentage\n",
    "    missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100\n",
    "    if missing_pct > 5:\n",
    "        issues.append(f\"{missing_pct:.1f}% missing values\")\n",
    "    \n",
    "    quality_issues[name] = issues\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    if issues:\n",
    "        for issue in issues:\n",
    "            print(f\"{issue}\")\n",
    "    else:\n",
    "        print(\"No quality issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18742ee2",
   "metadata": {},
   "source": [
    "## Baseline Model Performance\n",
    "\n",
    "How good are the classification results that a simple Random Forest Classifier can achieve on each data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7394c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Performance Evaluation:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    target_col = target_info[name]\n",
    "    \n",
    "    if not target_col:\n",
    "        print(f\"\\n{name.upper()}: Skipped (no target variable)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare features and target\n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Clean target variable (handle byte strings and other issues)\n",
    "        if y.dtype == 'object':\n",
    "            # Convert byte strings to regular strings\n",
    "            y = y.astype(str)\n",
    "            # Remove b' prefix and ' suffix if present\n",
    "            y = y.str.replace(r\"^b'|'$\", \"\", regex=True)\n",
    "            # Strip whitespace\n",
    "            y = y.str.strip()\n",
    "        \n",
    "        print(f\"  Target values: {sorted(y.unique())}\")\n",
    "        \n",
    "        # Simple preprocessing for baseline\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            # Same cleaning for categorical features\n",
    "            if X_processed[col].dtype == 'object':\n",
    "                X_processed[col] = X_processed[col].astype(str)\n",
    "                X_processed[col] = X_processed[col].str.replace(r\"^b'|'$\", \"\", regex=True)\n",
    "                X_processed[col] = X_processed[col].str.strip()\n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
    "        \n",
    "        # Ensure all columns are numeric\n",
    "        X_processed = X_processed.select_dtypes(include=[np.number])\n",
    "        \n",
    "        if X_processed.empty:\n",
    "            print(\"No usable features after preprocessing\")\n",
    "            continue\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_processed, y, test_size=0.2, random_state=42, \n",
    "            stratify=y if len(np.unique(y)) > 1 else None\n",
    "        )\n",
    "        \n",
    "        # Train Random Forest baseline\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = rf.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        unique_classes = len(np.unique(y))\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        if unique_classes == 2:\n",
    "            # For binary classification, use macro average to avoid pos_label issues\n",
    "            precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            # For multiclass, use macro average\n",
    "            precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        \n",
    "        # Store results\n",
    "        baseline_results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'features_used': X_processed.shape[1],\n",
    "            'original_features': X.shape[1]\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"  Features: {X_processed.shape[1]}/{X.shape[1]} usable\")\n",
    "        print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1-Score:  {f1:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce138d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_results:\n",
    "    # Create performance comparison plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Baseline Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "        row = idx // 2\n",
    "        col = idx % 2\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        names = list(baseline_results.keys())\n",
    "        values = [baseline_results[name][metric] for name in names]\n",
    "        colors = ['lightblue' if name in real_datasets else 'lightcoral' for name in names]\n",
    "        \n",
    "        bars = ax.bar(names, values, color=colors)\n",
    "        ax.set_title(metric_name)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efaae4",
   "metadata": {},
   "source": [
    "Results:\n",
    "- Among the real data sets, \"bank\" and \"income\" seem to be the easiest data sets for prediction\n",
    "- Among the synthetic data sets, the classification difficulty seems to increase along A -> B -> C\n",
    "- A simple Random Forest is already quite strong and can achieve accuracy scores of up to 94%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
