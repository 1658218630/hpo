{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af89f248",
   "metadata": {},
   "source": [
    "# Tournament Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8107dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup and Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to Python path\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "# Visualization setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c777bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tournament configuration\n",
    "CONFIG = {\n",
    "    'max_rounds': 300,\n",
    "    'max_parallel_workers': 4,  # Set to 1 to disable multithreading for debugging\n",
    "    'evaluation_metric': 'f1_score',  # 'accuracy', 'f1_score', 'precision', 'recall'\n",
    "    'cross_validation_folds': 3,\n",
    "    'sample_size': None,\n",
    "    'preprocessing_report': False,\n",
    "    'debug_mode': False,  # Enable detailed logging and disable threading\n",
    "    \n",
    "    # Multi-seed configuration\n",
    "    'num_seeds': 5,  # Number of different random seeds to test\n",
    "    'seeds': None,   # Will be auto-generated if None\n",
    "    'parallel_seeds': True,  # Run seeds in parallel\n",
    "    'seed_aggregation': 'mean',  # How to aggregate across seeds: 'mean', 'median'\n",
    "}\n",
    "\n",
    "# Available datasets\n",
    "AVAILABLE_DATASETS = {\n",
    "    'bank': os.path.join('..', 'data', 'real', 'bank.csv'),\n",
    "    'credit': os.path.join('..', 'data', 'real', 'credit.csv'),\n",
    "    'income': os.path.join('..', 'data', 'real', 'income.csv'),\n",
    "    'train_A': os.path.join('..', 'data', 'synthetic', 'train_A.csv'),\n",
    "    'train_B': os.path.join('..', 'data', 'synthetic', 'train_B.csv'),\n",
    "    'train_C': os.path.join('..', 'data', 'synthetic', 'train_C.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dynamic Model and Optimizer Loading\n",
    "def load_models():\n",
    "    \"\"\"Automatically load all models from the models directory (including subdirectories)\"\"\"\n",
    "    models = {}\n",
    "    models_dir = os.path.join('..', 'src', 'models')\n",
    "    \n",
    "    for root, dirs, files in os.walk(models_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.py') and not file.startswith('__') and file != 'BaseModel.py':\n",
    "                # Get relative path from models directory\n",
    "                rel_path = os.path.relpath(root, models_dir)\n",
    "                module_name = file[:-3]  # Remove .py extension\n",
    "                \n",
    "                # Build module import path\n",
    "                if rel_path == '.':\n",
    "                    import_path = f'models.{module_name}'\n",
    "                else:\n",
    "                    import_path = f'models.{rel_path.replace(os.sep, \".\")}.{module_name}'\n",
    "                \n",
    "                try:\n",
    "                    module = importlib.import_module(import_path)\n",
    "                    \n",
    "                    # Look for classes that are actually defined in this module\n",
    "                    found_class = None\n",
    "                    for attr_name in dir(module):\n",
    "                        attr = getattr(module, attr_name)\n",
    "                        if (isinstance(attr, type) and \n",
    "                            not attr_name.startswith('_') and\n",
    "                            attr_name != 'BaseModel' and\n",
    "                            # Check if class is defined in this module\n",
    "                            attr.__module__ == module.__name__):\n",
    "                            \n",
    "                            # Check if it's likely a model class\n",
    "                            if (attr_name.endswith('Model') or \n",
    "                                attr_name == module_name or\n",
    "                                'Model' in attr_name):\n",
    "                                found_class = attr\n",
    "                                break\n",
    "                    \n",
    "                    if found_class:\n",
    "                        clean_name = module_name.replace('Model', '') if module_name.endswith('Model') else module_name\n",
    "                        # Add subfolder prefix if in subdirectory\n",
    "                        if rel_path != '.':\n",
    "                            clean_name = f\"{rel_path.replace(os.sep, '_')}_{clean_name}\"\n",
    "                        models[clean_name] = found_class\n",
    "                        print(f\"Loaded model: {clean_name} (class: {found_class.__name__})\")\n",
    "                    else:\n",
    "                        print(f\"No model class found in {import_path}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load model {import_path}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def load_optimizers():\n",
    "    \"\"\"Automatically load all optimizers from the optimizers directory (including subdirectories)\"\"\"\n",
    "    optimizers = {}\n",
    "    optimizers_dir = os.path.join('..', 'src', 'optimizers')\n",
    "    \n",
    "    for root, dirs, files in os.walk(optimizers_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.py') and not file.startswith('__') and file != 'BaseOptimizer.py':\n",
    "                # Get relative path from optimizers directory\n",
    "                rel_path = os.path.relpath(root, optimizers_dir)\n",
    "                module_name = file[:-3]  # Remove .py extension\n",
    "                \n",
    "                # Build module import path\n",
    "                if rel_path == '.':\n",
    "                    import_path = f'optimizers.{module_name}'\n",
    "                else:\n",
    "                    import_path = f'optimizers.{rel_path.replace(os.sep, \".\")}.{module_name}'\n",
    "                \n",
    "                try:\n",
    "                    module = importlib.import_module(import_path)\n",
    "                    \n",
    "                    # Look for classes that are actually defined in this module\n",
    "                    found_class = None\n",
    "                    for attr_name in dir(module):\n",
    "                        attr = getattr(module, attr_name)\n",
    "                        if (isinstance(attr, type) and \n",
    "                            not attr_name.startswith('_') and\n",
    "                            attr_name != 'BaseOptimizer' and\n",
    "                            # Check if class is defined in this module\n",
    "                            attr.__module__ == module.__name__):\n",
    "                            \n",
    "                            # Check if it's likely an optimizer class\n",
    "                            if (attr_name.endswith('Optimizer') or \n",
    "                                attr_name == module_name or\n",
    "                                'Optimizer' in attr_name):\n",
    "                                found_class = attr\n",
    "                                break\n",
    "                    \n",
    "                    if found_class:\n",
    "                        clean_name = module_name.replace('Optimizer', '') if module_name.endswith('Optimizer') else module_name\n",
    "                        # Add subfolder prefix if in subdirectory\n",
    "                        if rel_path != '.':\n",
    "                            clean_name = f\"{rel_path.replace(os.sep, '_')}_{clean_name}\"\n",
    "                        optimizers[clean_name] = found_class\n",
    "                        print(f\"Loaded optimizer: {clean_name} (class: {found_class.__name__})\")\n",
    "                    else:\n",
    "                        print(f\"No optimizer class found in {import_path}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load optimizer {import_path}: {e}\")\n",
    "    \n",
    "    return optimizers\n",
    "\n",
    "print(\"Loading models...\")\n",
    "AVAILABLE_MODELS = load_models()\n",
    "\n",
    "print(\"Loading optimizers...\")\n",
    "AVAILABLE_OPTIMIZERS = load_optimizers()\n",
    "\n",
    "print(f\"Loaded {len(AVAILABLE_MODELS)} models and {len(AVAILABLE_OPTIMIZERS)} optimizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Loading and Preprocessing\n",
    "\n",
    "from data.preprocessing import preprocess_dataset\n",
    "from utils import TournamentResults, MultiSeedTournamentResults\n",
    "\n",
    "def load_and_preprocess_dataset(dataset_name: str, sample_size: int = None):\n",
    "    \"\"\"Load dataset with optional sampling for speed\"\"\"\n",
    "    X, y, features = preprocess_dataset(AVAILABLE_DATASETS[dataset_name], verbose=CONFIG[\"preprocessing_report\"])\n",
    "    \n",
    "    # Automatische Label-Konvertierung fÃ¼r alle Modelle\n",
    "    if hasattr(y, 'dtype') and y.dtype == 'object':  # String labels\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        print(f\"Converted string labels to numeric: {le.classes_}\")\n",
    "    \n",
    "    if sample_size and len(X) > sample_size:\n",
    "        indices = np.random.choice(len(X), sample_size, replace=False)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        print(f\"Sampled {sample_size} rows from {len(X)} total\")\n",
    "    \n",
    "    return X, y, features\n",
    "\n",
    "if CONFIG['seeds'] is None:\n",
    "    np.random.seed(42)  # FÃ¼r reproduzierbare Seed-Generierung\n",
    "    CONFIG['seeds'] = np.random.randint(1, 10000, CONFIG['num_seeds']).tolist()\n",
    "\n",
    "print(f\"Using seeds: {CONFIG['seeds']}\")\n",
    "\n",
    "# tournament_results = TournamentResults()\n",
    "tournament_results = MultiSeedTournamentResults(CONFIG['seeds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f7e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset (available: bank, credit, income, train_A, train_B, train_C)\n",
    "SELECTED_DATASET = 'train_B'\n",
    "\n",
    "# Select model (set to None to use first available)\n",
    "SELECTED_MODEL = \"EnhancedMLP\"  # e.g. 'RandomForest', 'MLP', 'SVM'\n",
    "\n",
    "# Select optimizers (set to None to use all available)\n",
    "SELECTED_OPTIMIZERS = ['rl_OptunaHybrid', 'rl_BayesianRL', 'rl_DQN', 'rl_EpsilonGreedy', 'rl_GradientBandit', 'rl_HybridBandit', 'rl_IndependentBandit', 'rl_OptunaHybrid', 'rl_OptunaTPE', 'rl_PPO']\n",
    "#[\"ea_ParticleSwarm\", \"ea_MuCommaLambdaES\", \"ea_CMAES\"]  e.g. ['rl_OptunaHybrid', 'RandomSearch', 'GridSearch'] or ['rl_DQN', 'rl_GradientBandit' 'ea_GeneticAlgorithm', 'rl_BayesianRL', 'ea_DifferentialEvolution', 'ea_EvolutionStrategyOptimizeer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab65863",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tournament Execution Engine\n",
    "\n",
    "def validate_tournament_configuration():\n",
    "    \"\"\"Validate and finalize tournament configuration\"\"\"\n",
    "    \n",
    "    print(\"\\nTournament Configuration\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Show available options\n",
    "    print(f\"Available datasets: {list(AVAILABLE_DATASETS.keys())}\")\n",
    "    print(f\"Available models: {list(AVAILABLE_MODELS.keys())}\")\n",
    "    print(f\"Available optimizers: {list(AVAILABLE_OPTIMIZERS.keys())}\")\n",
    "    \n",
    "    # Validate dataset\n",
    "    dataset = SELECTED_DATASET\n",
    "    if dataset not in AVAILABLE_DATASETS:\n",
    "        print(f\"Dataset '{dataset}' not found, using first available\")\n",
    "        dataset = list(AVAILABLE_DATASETS.keys())[0]\n",
    "    \n",
    "    # Validate model\n",
    "    model = SELECTED_MODEL\n",
    "    if model is None or model not in AVAILABLE_MODELS:\n",
    "        if model is not None:\n",
    "            print(f\"Model '{model}' not found, using first available\")\n",
    "        model = list(AVAILABLE_MODELS.keys())[0]\n",
    "    \n",
    "    # Validate optimizers\n",
    "    optimizers = SELECTED_OPTIMIZERS\n",
    "    if optimizers is None:\n",
    "        optimizers = list(AVAILABLE_OPTIMIZERS.keys())\n",
    "    else:\n",
    "        valid_optimizers = []\n",
    "        for opt in optimizers:\n",
    "            if opt in AVAILABLE_OPTIMIZERS:\n",
    "                valid_optimizers.append(opt)\n",
    "            else:\n",
    "                print(f\"Optimizer '{opt}' not found, skipping\")\n",
    "        optimizers = valid_optimizers if valid_optimizers else list(AVAILABLE_OPTIMIZERS.keys())\n",
    "    \n",
    "    print(f\"\\nFinal tournament configuration:\")\n",
    "    print(f\"  â€¢ Dataset: {dataset}\")\n",
    "    print(f\"  â€¢ Model: {model}\")\n",
    "    print(f\"  â€¢ Optimizers: {optimizers}\")\n",
    "    \n",
    "    return dataset, model, optimizers\n",
    "\n",
    "# Validate configuration\n",
    "selected_dataset, selected_model, selected_optimizers = validate_tournament_configuration()\n",
    "\n",
    "def evaluate_model_performance(model, X, y, hyperparameters: Dict, seed: int = None) -> float:\n",
    "    \"\"\"Evaluate model performance using cross-validation with optional seed\"\"\"\n",
    "    # Create and configure model\n",
    "    model.create_model(hyperparameters)\n",
    "    \n",
    "    # Use seed if provided, otherwise use default\n",
    "    random_state = seed if seed is not None else 42\n",
    "    cv = StratifiedKFold(n_splits=CONFIG['cross_validation_folds'], shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Choose scoring metric\n",
    "    scoring_map = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1_score': 'f1_macro',\n",
    "        'precision': 'precision_macro',\n",
    "        'recall': 'recall_macro'\n",
    "    }\n",
    "    scoring = scoring_map.get(CONFIG['evaluation_metric'], 'f1_macro')\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    try:\n",
    "        if CONFIG.get('debug_mode', False):\n",
    "            scores = cross_val_score(model.model, X, y, cv=cv, scoring=scoring, n_jobs=1)\n",
    "            if seed is not None:\n",
    "                print(f\"  CV scores (seed {seed}): {scores}\")\n",
    "            print(f\"  Mean CV score: {np.mean(scores):.4f}\")\n",
    "        else:\n",
    "            scores = cross_val_score(model.model, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        return float(np.mean(scores))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluation: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def run_single_tournament_round(optimizer_name: str, round_num: int, X, y, \n",
    "                               model_class, optimizer_instance, seed: int = None) -> Dict:\n",
    "    \"\"\"Run a single round for one optimizer, optionally with specific seed\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create fresh model instance\n",
    "        model_instance = model_class()\n",
    "        \n",
    "        # Set seed for this specific run if provided\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed + round_num + hash(optimizer_name) % 1000)\n",
    "        \n",
    "        # Get hyperparameter suggestion\n",
    "        suggested_params = optimizer_instance.suggest_hyperparameters(round_num)\n",
    "        \n",
    "        # Evaluate model performance\n",
    "        score = evaluate_model_performance(model_instance, X, y, suggested_params, seed)\n",
    "        \n",
    "        # Update optimizer with results\n",
    "        optimizer_instance.update(suggested_params, score)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'seed': seed,\n",
    "            'optimizer': optimizer_name,\n",
    "            'round': round_num,\n",
    "            'score': score,\n",
    "            'params': suggested_params,\n",
    "            'time': training_time\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'seed': seed,\n",
    "            'optimizer': optimizer_name,\n",
    "            'round': round_num,\n",
    "            'error': str(e),\n",
    "            'time': time.time() - start_time\n",
    "        }\n",
    "    \n",
    "def run_tournament_round(round_num: int, X, y, model_class, \n",
    "                        optimizer_instances: Dict, \n",
    "                        tournament_results: MultiSeedTournamentResults):\n",
    "    \"\"\"Run a complete round across all seeds and optimizers\"\"\"\n",
    "    \n",
    "    round_start_time = time.time()\n",
    "    \n",
    "    if CONFIG['parallel_seeds'] and not CONFIG.get('debug_mode', False):\n",
    "        # Parallel execution across seeds and optimizers\n",
    "        max_workers = min(CONFIG['max_parallel_workers'], len(CONFIG['seeds']) * len(optimizer_instances))\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            \n",
    "            # Submit tasks for each seed-optimizer combination\n",
    "            for seed in CONFIG['seeds']:\n",
    "                for optimizer_name, optimizer_instance in optimizer_instances.items():\n",
    "                    future = executor.submit(\n",
    "                        run_single_tournament_round,\n",
    "                        optimizer_name, round_num, X, y, \n",
    "                        model_class, optimizer_instance, seed\n",
    "                    )\n",
    "                    futures.append(future)\n",
    "            \n",
    "            # Process completed tasks\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result['success']:\n",
    "                    tournament_results.add_result(\n",
    "                        result['seed'], result['optimizer'], result['round'],\n",
    "                        result['params'], result['score'], result['time']\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"Error in seed {result['seed']}, optimizer {result['optimizer']}: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    else:\n",
    "        # Sequential execution for debugging\n",
    "        for seed in CONFIG['seeds']:\n",
    "            for optimizer_name, optimizer_instance in optimizer_instances.items():\n",
    "                if CONFIG.get('debug_mode', False):\n",
    "                    print(f\"Running seed {seed}, optimizer {optimizer_name}\")\n",
    "                \n",
    "                result = run_single_tournament_round(\n",
    "                    optimizer_name, round_num, X, y, \n",
    "                    model_class, optimizer_instance, seed\n",
    "                )\n",
    "                \n",
    "                if result['success']:\n",
    "                    tournament_results.add_result(\n",
    "                        result['seed'], result['optimizer'], result['round'],\n",
    "                        result['params'], result['score'], result['time']\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"Error: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    round_time = time.time() - round_start_time\n",
    "    return round_time\n",
    "\n",
    "## Live Visualization Functions\n",
    "\n",
    "def create_live_tournament_dashboard(show_text=False):\n",
    "    \"\"\"Create enhanced live dashboard with error bars and seed variance\"\"\"\n",
    "    \n",
    "    if not CONFIG[\"debug_mode\"]:\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    standings = tournament_results.get_current_standings_with_variance()\n",
    "    \n",
    "    if standings.empty:\n",
    "        print(\"No results yet...\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots (ohne groÃŸe Tabelle)\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    gs = fig.add_gridspec(1, 3, hspace=0.3, wspace=0.5)  # Nur eine Reihe\n",
    "    \n",
    "    # Main title\n",
    "    current_round = max([max(data['rounds']) if data['rounds'] else 0 \n",
    "                        for seed_data in tournament_results.seed_results.values() \n",
    "                        for data in seed_data.values()], default=0)\n",
    "    \n",
    "    fig.suptitle(f'Live Multi-Seed Tournament Dashboard - Round {current_round}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Performance comparison with error bars (verbessert fÃ¼r viele Optimizer)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    optimizers = standings['Optimizer'].values\n",
    "    means = standings['Mean Best Score'].values\n",
    "    stds = standings['Std Best Score'].values\n",
    "    \n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(optimizers)))\n",
    "    \n",
    "    # Bessere Y-Achsen Skalierung\n",
    "    y_min = max(0, min(means) - max(stds) - 0.02)\n",
    "    y_max = max(means) + max(stds) + 0.05  # Mehr Platz fÃ¼r Labels\n",
    "    \n",
    "    # VergrÃ¶ÃŸerte Fehlerbalken und bessere Sichtbarkeit\n",
    "    bars = ax1.bar(range(len(optimizers)), means, \n",
    "                   color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Separate Fehlerbalken fÃ¼r bessere Sichtbarkeit\n",
    "    ax1.errorbar(range(len(optimizers)), means, yerr=stds, \n",
    "                fmt='none', color='black', capsize=6, capthick=1.5, linewidth=1.5)\n",
    "    \n",
    "    ax1.set_xlabel('Optimizers', fontsize=12)\n",
    "    ax1.set_ylabel('Best Score (Mean Â± Std)', fontsize=12)\n",
    "    ax1.set_title('Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax1.set_xticks(range(len(optimizers)))\n",
    "    \n",
    "    # Bessere Label-Rotation basierend auf Anzahl\n",
    "    if len(optimizers) <= 4:\n",
    "        ax1.set_xticklabels(optimizers, rotation=0, ha='center')\n",
    "        label_fontsize = 9\n",
    "    elif len(optimizers) <= 8:\n",
    "        ax1.set_xticklabels(optimizers, rotation=45, ha='right')\n",
    "        label_fontsize = 8\n",
    "    else:\n",
    "        ax1.set_xticklabels(optimizers, rotation=90, ha='right')\n",
    "        label_fontsize = 7\n",
    "    \n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Intelligente Label-Positionierung um Ãœberlappungen zu vermeiden\n",
    "    for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "        cv = (std/mean*100) if mean > 0 else 0\n",
    "        \n",
    "        # Alterniere Label-Position bei vielen Optimizern\n",
    "        if len(optimizers) > 6:\n",
    "            offset = 0.015 if i % 2 == 0 else 0.025\n",
    "            label_text = f'{mean:.3f}\\nÂ±{std:.3f}'  # Kompakter\n",
    "        else:\n",
    "            offset = 0.015\n",
    "            label_text = f'{mean:.3f}Â±{std:.3f}\\n({cv:.1f}%)'\n",
    "        \n",
    "        ax1.text(i, mean + std + offset, label_text, \n",
    "                ha='center', va='bottom', fontsize=label_fontsize, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # 2. Score progression over rounds (verbessert fÃ¼r mehr Optimizer)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    # Zeige alle Optimizer, aber mit intelligenteren Linien\n",
    "    max_optimizers_to_show = min(len(optimizers), 8)  # Maximal 8 statt 4\n",
    "    \n",
    "    for i, optimizer in enumerate(optimizers[:max_optimizers_to_show]):\n",
    "        score_history = tournament_results.get_score_history_by_seed(optimizer)\n",
    "        \n",
    "        # Calculate statistics across seeds for each round\n",
    "        max_rounds = max(len(scores) for scores in score_history.values() if scores)\n",
    "        if max_rounds > 0:\n",
    "            round_means = []\n",
    "            round_stds = []\n",
    "            \n",
    "            for round_idx in range(max_rounds):\n",
    "                round_scores = []\n",
    "                for seed_scores in score_history.values():\n",
    "                    if round_idx < len(seed_scores):\n",
    "                        round_scores.append(seed_scores[round_idx])\n",
    "                \n",
    "                if round_scores:\n",
    "                    round_means.append(np.mean(round_scores))\n",
    "                    round_stds.append(np.std(round_scores))\n",
    "                else:\n",
    "                    round_means.append(np.nan)\n",
    "                    round_stds.append(np.nan)\n",
    "            \n",
    "            rounds = range(1, len(round_means) + 1)\n",
    "            round_means = np.array(round_means)\n",
    "            round_stds = np.array(round_stds)\n",
    "            \n",
    "            # DÃ¼nnere Linien und verschiedene Stile fÃ¼r bessere Unterscheidung\n",
    "            line_styles = ['-', '--', '-.', ':']\n",
    "            line_style = line_styles[i % len(line_styles)]\n",
    "            line_width = 2.0 if i < 4 else 1.5  # Top 4 dicker\n",
    "            alpha = 1.0 if i < 4 else 0.8  # Top 4 mehr opacity\n",
    "            \n",
    "            # Plot mean line\n",
    "            ax2.plot(rounds, round_means, label=optimizer, color=colors[i], \n",
    "                    linewidth=line_width, linestyle=line_style, alpha=alpha, marker='o', markersize=3)\n",
    "            \n",
    "            # Add confidence interval nur fÃ¼r Top 4 um Clutter zu vermeiden\n",
    "            if i < 4:\n",
    "                ax2.fill_between(rounds, \n",
    "                               round_means - round_stds, \n",
    "                               round_means + round_stds, \n",
    "                               alpha=0.2, color=colors[i])\n",
    "    \n",
    "    ax2.set_xlabel('Round', fontsize=12)\n",
    "    ax2.set_ylabel('Score', fontsize=12)\n",
    "    ax2.set_title('Score Progression', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Bessere Legende basierend auf Anzahl Optimizer\n",
    "    if len(optimizers) <= 4:\n",
    "        ax2.legend(loc='upper left', fontsize=10)\n",
    "    elif len(optimizers) <= 6:\n",
    "        ax2.legend(loc='upper left', fontsize=9, ncol=2)\n",
    "    else:\n",
    "        ax2.legend(loc='upper left', fontsize=8, ncol=2)\n",
    "    \n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Reproducibility Analysis (kompakter)\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    \n",
    "    # Berechne Coefficient of Variation fÃ¼r jeden Optimizer\n",
    "    cv_values = []\n",
    "    optimizer_names_cv = []\n",
    "    \n",
    "    for _, row in standings.iterrows():\n",
    "        if row['Mean Best Score'] > 0:\n",
    "            cv = (row['Std Best Score'] / row['Mean Best Score']) * 100\n",
    "            cv_values.append(cv)\n",
    "            optimizer_names_cv.append(row['Optimizer'])\n",
    "    \n",
    "    if cv_values:\n",
    "        # Sortiere nach CV (niedrigster = reproducible)\n",
    "        sorted_data = sorted(zip(cv_values, optimizer_names_cv))\n",
    "        cv_values, optimizer_names_cv = zip(*sorted_data)\n",
    "        \n",
    "        colors_cv = ['green' if cv < 5 else 'orange' if cv < 10 else 'red' for cv in cv_values]\n",
    "        \n",
    "        bars_cv = ax3.barh(range(len(cv_values)), cv_values, color=colors_cv, alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        ax3.set_yticks(range(len(optimizer_names_cv)))\n",
    "        ax3.set_yticklabels(optimizer_names_cv, fontsize=10)\n",
    "        ax3.set_xlabel('Coefficient of Variation (%)', fontsize=12)\n",
    "        ax3.set_title('Reproducibility Analysis', fontsize=14, fontweight='bold')\n",
    "        ax3.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, cv in enumerate(cv_values):\n",
    "            ax3.text(cv + 0.05, i, f'{cv:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Kompakte Legende innerhalb des Plots\n",
    "        legend_elements = [\n",
    "            plt.Rectangle((0,0),1,1, facecolor='green', alpha=0.7, label='<5%'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='orange', alpha=0.7, label='5-10%'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.7, label='>10%')\n",
    "        ]\n",
    "        ax3.legend(handles=legend_elements, loc='lower right', fontsize=9, title='CV')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Kompakte Textausgabe statt groÃŸe Tabelle\n",
    "    if show_text:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TOURNAMENT SUMMARY - Round {current_round}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Seeds: {len(CONFIG['seeds'])} | Metric: {CONFIG['evaluation_metric']}\")\n",
    "        \n",
    "        if not standings.empty:\n",
    "            print(f\"\\nTOP 3 PERFORMERS:\")\n",
    "            for i, (_, row) in enumerate(standings.head(3).iterrows()):\n",
    "                cv = (row['Std Best Score']/row['Mean Best Score']*100) if row['Mean Best Score'] > 0 else 0\n",
    "                medal = \"ðŸ¥‡\" if i == 0 else \"ðŸ¥ˆ\" if i == 1 else \"ðŸ¥‰\"\n",
    "                print(f\"  {medal} {row['Optimizer']}: {row['Mean Best Score']:.4f}Â±{row['Std Best Score']:.4f} (CV: {cv:.1f}%)\")\n",
    "            \n",
    "            # ZusÃ¤tzliche Insights\n",
    "            winner = standings.iloc[0]\n",
    "            winner_cv = (winner['Std Best Score']/winner['Mean Best Score']*100) if winner['Mean Best Score'] > 0 else 0\n",
    "            \n",
    "            print(f\"\\nðŸ“Š INSIGHTS:\")\n",
    "            print(f\"   â€¢ Winner Reproducibility: {'Excellent' if winner_cv < 5 else 'Good' if winner_cv < 10 else 'Poor'}\")\n",
    "            print(f\"   â€¢ Total Evaluations: {winner['Total Evaluations']}\")\n",
    "            print(f\"   â€¢ Seeds per Optimizer: {winner['Seeds Completed']}/{len(CONFIG['seeds'])}\")\n",
    "    \n",
    "    # Print summary statistics if requested\n",
    "    if show_text:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MULTI-SEED TOURNAMENT STATISTICS - Round {current_round}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total Seeds: {len(CONFIG['seeds'])}\")\n",
    "        print(f\"Seeds: {CONFIG['seeds']}\")\n",
    "        \n",
    "        if not standings.empty:\n",
    "            print(f\"\\nCURRENT LEADER:\")\n",
    "            leader = standings.iloc[0]\n",
    "            print(f\"  {leader['Optimizer']}\")\n",
    "            print(f\"    Mean Best Score: {leader['Mean Best Score']:.4f} Â± {leader['Std Best Score']:.4f}\")\n",
    "            print(f\"    Seeds Completed: {leader['Seeds Completed']}/{leader['Total Seeds']}\")\n",
    "            \n",
    "            print(f\"\\nTOP 3 PERFORMERS:\")\n",
    "            for _, row in standings.head(3).iterrows():\n",
    "                print(f\"  {row['Rank']}. {row['Optimizer']}: {row['Mean Best Score']:.4f} Â± {row['Std Best Score']:.4f}\")\n",
    "\n",
    "def print_round_status(round_num, max_rounds, round_time):\n",
    "    \"\"\"Print round status with seed information\"\"\"\n",
    "    seed_info = f\" ({len(CONFIG['seeds'])} seeds)\" if CONFIG['num_seeds'] > 1 else \"\"\n",
    "    print(f\"Round {round_num}/{max_rounds}{seed_info} - {round_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(f\"Loading dataset: {selected_dataset}\")\n",
    "X, y, feature_names = load_and_preprocess_dataset(selected_dataset, sample_size=CONFIG[\"sample_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tournament Execution\n",
    "\n",
    "print(f\"\\nStarting Enhanced Tournament!\")\n",
    "print(f\"Max rounds: {CONFIG['max_rounds']}\")\n",
    "print(f\"Seeds: {CONFIG['num_seeds']} ({CONFIG['seeds']})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize model and optimizer instances\n",
    "model_class = AVAILABLE_MODELS[selected_model]\n",
    "model_instance = AVAILABLE_MODELS[selected_model]()\n",
    "optimizer_instances = {}\n",
    "\n",
    "for optimizer_name in selected_optimizers:\n",
    "    optimizer_instances[optimizer_name] = AVAILABLE_OPTIMIZERS[optimizer_name](\n",
    "        model_instance.hyperparameter_space\n",
    "    )\n",
    "\n",
    "print(f\"\\nInitialized {len(optimizer_instances)} optimizers\")\n",
    "\n",
    "# Main tournament loop\n",
    "for round_num in range(1, CONFIG['max_rounds'] + 1):\n",
    "    \n",
    "    # Run round across all seeds\n",
    "    round_time = run_tournament_round(round_num, X, y, model_class, optimizer_instances, tournament_results)\n",
    "    \n",
    "    # Update dashboard every round\n",
    "    if not CONFIG.get('debug_mode', False):\n",
    "        create_live_tournament_dashboard(show_text=False)\n",
    "        print_round_status(round_num, CONFIG['max_rounds'], round_time)\n",
    "    else:\n",
    "        print(f\"\\nRound {round_num} completed in {round_time:.1f}s\")\n",
    "        # Show current standings in debug mode\n",
    "        standings = tournament_results.get_current_standings_with_variance()\n",
    "        if not standings.empty:\n",
    "            print(\"Current standings:\")\n",
    "            for _, row in standings.head(3).iterrows():\n",
    "                print(f\"  {row['Rank']}. {row['Optimizer']}: {row['Mean Best Score']:.4f} Â± {row['Std Best Score']:.4f}\")\n",
    "    \n",
    "    # Show progress every 10 rounds\n",
    "    if round_num % 10 == 0:\n",
    "        standings = tournament_results.get_current_standings_with_variance()\n",
    "        if not standings.empty:\n",
    "            leader = standings.iloc[0]\n",
    "            print(f\"Current leader: {leader['Optimizer']} \"\n",
    "                  f\"({leader['Mean Best Score']:.4f} Â± {leader['Std Best Score']:.4f})\")\n",
    "\n",
    "print(f\"\\nTournament Complete!\")\n",
    "\n",
    "## Final Results and Analysis\n",
    "\n",
    "print(\"FINAL TOURNAMENT RESULTS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Show final dashboard with text details\n",
    "create_live_tournament_dashboard(show_text=True)\n",
    "\n",
    "# Detailed final standings\n",
    "final_standings = tournament_results.get_current_standings_with_variance()\n",
    "\n",
    "if not final_standings.empty:\n",
    "    print(f\"\\nTOURNAMENT WINNER:\")\n",
    "    winner = final_standings.iloc[0]\n",
    "    print(f\"  {winner['Optimizer']}\")\n",
    "    print(f\"     Mean Best Score: {winner['Mean Best Score']:.4f} Â± {winner['Std Best Score']:.4f}\")\n",
    "    print(f\"     Seeds Completed: {winner['Seeds Completed']}/{winner['Total Seeds']}\")\n",
    "    print(f\"     Total Evaluations: {winner['Total Evaluations']}\")\n",
    "    \n",
    "    # Reproducibility analysis\n",
    "    cv = (winner['Std Best Score']/winner['Mean Best Score']*100) if winner['Mean Best Score'] > 0 else 0\n",
    "    print(f\"     Coefficient of Variation: {cv:.2f}%\")\n",
    "\n",
    "    print(f\"\\nTOP 3 PERFORMERS:\")\n",
    "    for _, row in final_standings.head(3).iterrows():\n",
    "        cv = (row['Std Best Score']/row['Mean Best Score']*100) if row['Mean Best Score'] > 0 else 0\n",
    "        print(f\"  Rank {row['Rank']}: {row['Optimizer']} - {row['Mean Best Score']:.4f}Â±{row['Std Best Score']:.4f} (CV: {cv:.1f}%)\")\n",
    "\n",
    "# Best hyperparameters analysis (erweitert fÃ¼r Multi-Seed)\n",
    "print(f\"\\nBEST HYPERPARAMETERS ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for optimizer in final_standings['Optimizer'].head(3):  # Top 3 only\n",
    "    # Get best hyperparams across all seeds for this optimizer\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    for seed_data in tournament_results.seed_results.values():\n",
    "        if optimizer in seed_data and seed_data[optimizer]['best_hyperparams']:\n",
    "            if seed_data[optimizer]['best_score'] > best_score:\n",
    "                best_score = seed_data[optimizer]['best_score']\n",
    "                best_params = seed_data[optimizer]['best_hyperparams']\n",
    "    \n",
    "    if best_params:\n",
    "        print(f\"\\n{optimizer}:\")\n",
    "        print(f\"  Best Score: {best_score:.4f}\")\n",
    "        print(\"  Best hyperparameters:\")\n",
    "        for param, value in best_params.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"    {param}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"    {param}: {value}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TOURNAMENT COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
