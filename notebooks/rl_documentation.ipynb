{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016b6b22",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimierung mit Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f1c34",
   "metadata": {},
   "source": [
    "## Reinforcement Learning als allgemeiner Prozess\n",
    "\n",
    "Reinforcement Learning (RL) ist eine Unterkategorie des Maschinellen Lernens, bei dem ein Agent durch Interaktion mit seiner Umwelt versucht optimale Entscheidungen zu treffen. Im Allgemeinen wird dieser Prozess als Markov Decision Process abgebildet, bei dem folgende Elemente existieren:  \n",
    "- states $s$ (in einer Environment)\n",
    "- actions $a$ (eines Agenten)\n",
    "- reward model $r(s,a)$\n",
    "- transition model $p(s'|s,a)$\n",
    "- initial state distribution $\\mu_0(s)$\n",
    "\n",
    "![](images/rl/markov_decision_process.PNG)\n",
    "\n",
    "Die Markov Eigenschaft besagt dabei, dass die zukünftigen Zustände nur vom aktuellen Zustand und der gewählten Aktion abhängen.  \n",
    "\n",
    "\n",
    "$p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},s_{t-2}) = p(s_{t+1}|s_t,a_t)$\n",
    "\n",
    "\n",
    "Die Policy $\\pi(a|s)$ beschreibt außerdem die Wahl des Agenten einer passenden Aktion bei einem definierten Zustand. Diese Policy optimal zu lernen ist das Endziel eines jeden Reinforcement Learning Algorithmus.\n",
    "\n",
    "## Anwendung auf die Hyperparameter Optimierung\n",
    "\n",
    "Bei der Hyperparemeter Optimierung werden die Elemente des Markov Decision Process und des RL wie folgt zugewiesen:\n",
    "\n",
    "|Markov Decision Process / RL |Hyperparemeter Optimierung |\n",
    "|-----------------------------|---------------------------|\n",
    "|Agent                        |Optimierer                 |\n",
    "|Environment                  |Modell                     |\n",
    "|state                        |Satz an Hyperparemetern    |\n",
    "|actions                      |Auswahl von Hyperparemetern|\n",
    "|rewards                      |Performance des Modells    |\n",
    "|transition model             |Deterministisch und damit nicht relevant|\n",
    "|initial state distribution   |Zufälliger Satz an Hyperparametern zu Beginn|\n",
    "\n",
    "Der Training Loop des RL Optimizers lässt sich also wie folgt beschreiben:\n",
    "\n",
    "![](images/rl/training_loop.png)\n",
    "\n",
    "- Zunächst wähl der RL Optmizer die Hyperparameter des Modells. In der ersten Iteration geschieht dies zufällig, da noch keine Informationen über die Environment (das Modell) bekannt sind.\n",
    "- Anschließend wird das Modell mit den gewählten Hyperparematern getestet. Das heißt das Modell trainiert mit dem gewählten Datensatz und den gegebenen Hyperparemetern.\n",
    "- Durch einen Train-Test-Split wird nun die Performance des Modells in einer definierten Metrik, wie z.B. des f1-scores bestimmt.\n",
    "- Diese Performance geht als reward zurück zum Optimizer und dieser kann auf Grund des Feedbacks neue Entscheidungen treffen.\n",
    "- Der Prozess beginn von Vorne "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
