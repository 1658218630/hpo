{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f341efd",
   "metadata": {},
   "source": [
    "# Data Analytics for the \"Credit\" Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c28b2",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Loading the Data](#1-loading-the-data)\n",
    "2. [Data Set Size](#2-data-set-size)\n",
    "3. [Miscellaneous Observations on the Data](#3-miscellaneous-observations-on-the-data)\n",
    "4. [Class Distribution](#4-class-distribution)\n",
    "5. [Distribution of Categorical Features](#5-distribution-of-categorical-features)\n",
    "6. [Distribution of Numerical Features](#6-distribution-of-numerical-features)\n",
    "7. [Averages Along Numerical Features](#7-averages-along-numerical-features)\n",
    "8. [Feature Correlations](#8-feature-correlations)\n",
    "    - [Correlation loan_amount, loan_duration and target Variable](#81-correlation-loan_amount-loan_duration-and-target-variable)\n",
    "    - [Correlation loan_amount, purpose/ age and target Variable](#82-correlation-loan_amount-purpose-age-and-target-variable)\n",
    "9. [Applying Random Forest Classifier to Assess Feature Importance](#9-applying-random-forest-classifier-to-assess-feature-importance)\n",
    "10. [Summary of Potential Data Flaws](#10-summary-of-potential-data-flaws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb26b2",
   "metadata": {},
   "source": [
    "### 1. Loading the Data\n",
    "\n",
    "Here, we are working with the [German Credit Data Set from the UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
    "The data was originally provided by Prof. Hans Hofmann.\n",
    "The UCI repository furthermore states that \"It is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\" (see [here](https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data)), meaning that a high recall is eventually more important than a high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "credit_data = pd.read_csv(os.path.join('..', 'data', 'real', 'credit.csv'))\n",
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a60c8a",
   "metadata": {},
   "source": [
    "### 2. Data Set Size\n",
    "\n",
    "First, we want to explore how many data points are available for training the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = credit_data.shape[1]\n",
    "num_samples = credit_data.shape[0]\n",
    "\n",
    "print(f\"Number of features: {num_features}\")\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "\n",
    "target_col = 'target'\n",
    "numerical_cols = credit_data.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = credit_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(f\"Number of numerical columns: {len(numerical_cols)}\")\n",
    "print(f\"Number of categorical columns: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86117eda",
   "metadata": {},
   "source": [
    "Result: In contrast to \"bank\" or \"income\", this data set has noticeably less samples. We thus need to utilize the available data even better. With 21 features, 7 numerical and 14 categorical ones (one of which is the label), we have enough features to make a profound feature selection essential for the success of a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486eda3e",
   "metadata": {},
   "source": [
    "### 3. Miscellaneous Observations on the Data\n",
    "\n",
    "Ensuring data integrity by checking for null values, duplicate rows and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert byte strings to normal strings for all object columns\n",
    "print(f\"Element before conversion: {credit_data[categorical_cols[1]][3]}\")\n",
    "for col in credit_data.select_dtypes(include=['object']).columns:\n",
    "    credit_data[col] = credit_data[col].astype(str).str.replace(r\"^b'|'$\", \"\", regex=True).str.strip()\n",
    "print(f\"Element after conversion: {credit_data[categorical_cols[1]][3]}\")\n",
    "\n",
    "num_nulls = credit_data.isnull().sum().sum()\n",
    "print(f\"Number of null values: {num_nulls}\")\n",
    "\n",
    "duplicates = credit_data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1161d",
   "metadata": {},
   "source": [
    "Results:\n",
    "- The categorical feature values of this data set are provided as byte-strings. To make working with them easier, they are converted to normal strings here\n",
    "- The data set does not have any missing values -> no extra preprocessing needed here\n",
    "- The data set does not have any duplicated samples either -> no extra preprocessing needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70706d4",
   "metadata": {},
   "source": [
    "### 4. Class Distribution\n",
    "\n",
    "Do we have to account for a class imbalance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='target', data=credit_data)\n",
    "\n",
    "plt.title('Distribution of the Target Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247c49c",
   "metadata": {},
   "source": [
    "Result: There is some class imbalance towards credit applications that were assessed as 'good' (credit was granted). This needs to be addressed during model training, however there are still enough 'bad' samples available for a good assessment of patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14be3c9",
   "metadata": {},
   "source": [
    "### 5. Distribution of Categorical Features\n",
    "\n",
    "How many distinct values does each categorical feature have? How is the distribution along these of the given 1000 samples? Are there obvious correlations between certain values and the target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29799bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(categorical_cols)\n",
    "ncols = 3\n",
    "nrows = math.ceil(n / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12, 4 * nrows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    if col == target_col:\n",
    "        continue\n",
    "    ax = axes[idx]\n",
    "    counts = credit_data.groupby([col, target_col]).size().unstack(fill_value=0)\n",
    "    counts.plot(kind='bar', stacked=True, color=['coral', 'skyblue'], ax=ax, width=0.8)\n",
    "    ax.set_title(col)\n",
    "    ax.set_ylabel('count')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(idx, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "red_patch = mpatches.Patch(color='coral', label='bad')\n",
    "blue_patch = mpatches.Patch(color='skyblue', label='good')\n",
    "fig.legend(handles=[red_patch, blue_patch], loc='upper right', fontsize=12, title=target_col)\n",
    "\n",
    "plt.suptitle('Histograms of Categorical Features - Stacked by Assessment Result', x=0.5, y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0540ac4",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "- None of the features has more than 10 possible values -> categorical value encoding in a compact space possible\n",
    "- Luckily for us, potential null values are already encoded: e.g. by 'other' or 'no known savings'\n",
    "- Some values are only represented by very few samples (e.g. is_foreign=no or guarantor other than none) -> hard to tell in how far these values actually influence the target variable\n",
    "- People without a checking account at this bank have far worse chances of getting a credit\n",
    "- Getting money for a radio/ TV is no problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a4ab6",
   "metadata": {},
   "source": [
    "### 6. Distribution of Numerical Features\n",
    "\n",
    "What values do the numerical features have and how are they distributed in their feature space? Are there obvious correlations between certain values and the target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1437948",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_int_count = 0\n",
    "for col in numerical_cols:\n",
    "    non_int_count = non_int_count + credit_data[col].dropna().apply(lambda x: not x.is_integer()).sum()\n",
    "print(f\"Total non-integer values in numerical columns: {non_int_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9eceb",
   "metadata": {},
   "source": [
    "Result: Even though all numerical features are provided as floats, they are in fact all integers (always ending on '.0')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(numerical_cols)\n",
    "ncols = 3\n",
    "nrows = math.ceil(n / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12, 4 * nrows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = {'bad': 'coral', 'good': 'skyblue'}\n",
    "target_values = credit_data[target_col].unique()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    ax = axes[idx]\n",
    "    for t in target_values:\n",
    "        data = credit_data[credit_data[target_col] == t][col].dropna()\n",
    "        ax.hist(data, bins=20, alpha=0.7, label=str(t), color=colors.get(str(t), 'gray'), stacked=True)\n",
    "    ax.set_title(col)\n",
    "    ax.set_ylabel('count')\n",
    "    ax.set_xlabel('')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(idx + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "red_patch = mpatches.Patch(color='coral', label='bad')\n",
    "blue_patch = mpatches.Patch(color='skyblue', label='good')\n",
    "fig.legend(handles=[red_patch, blue_patch], loc='upper right', fontsize=12, title=target_col)\n",
    "\n",
    "plt.suptitle('Histograms of Numerical Features - Stacked by Assessment Result', x=0.5, y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da492661",
   "metadata": {},
   "source": [
    "Conclusions (among others):\n",
    "- 'Dependents' is in fact more of a binary feature\n",
    "- Other features like 'num_existing_loans' and 'residence_years' seem to have rather distinct values, too\n",
    "- Short loans are rejected less than long loans\n",
    "- Very small loans are barely rejected, but small loans are rejected the most, but very large loans (above 12.000â‚¬) are mostly accepted (maybe because applying for these big loans is done rather carefully)\n",
    "- Very old applicants seem to have lower loan acceptance rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56848462",
   "metadata": {},
   "source": [
    "### 7. Averages Along Numerical Features\n",
    "\n",
    "What are the key facts of the average loan application? What about the same facts about the average accepted loan application?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive statistics for the numerical features of all credit applications:\")\n",
    "credit_data.describe().loc[['count', 'mean', 'std', 'min', 'max']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Descriptive statistics for the numerical features of successful credit applications:\")\n",
    "credit_data[credit_data['target'] == 'good'].describe().loc[['count', 'mean', 'std', 'min', 'max']].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c13caf",
   "metadata": {},
   "source": [
    "Conclusions on the data: Successful applicants tend to be slightly older, request lower loan amounts, and opt for shorter loan durations compared to the overall applicant pool. Other features such as number of dependents, residence years, and installment rate show little difference between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518558a0",
   "metadata": {},
   "source": [
    "### 8. Feature Correlations\n",
    "\n",
    "Are there any redundant features? Are there any features with noticeably high correlations to the target variable (great for prediction)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224557de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Make a copy to avoid changing your original data\n",
    "credit_data_numeric = credit_data.copy()\n",
    "\n",
    "# Encode all categorical columns\n",
    "for col in credit_data_numeric.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    credit_data_numeric[col] = le.fit_transform(credit_data_numeric[col])\n",
    "\n",
    "# Now compute the correlation matrix including categorical features\n",
    "corr_matrix = credit_data_numeric.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "    square=True, fmt='.2f', cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "plt.title('Correlation Matrix (Including Encoded Categorical Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705320b",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "- No duplicate features\n",
    "- Correlation between loan_duration and loan_amount\n",
    "- Correlation between num_existing_loans and repayment_history: repayment_history e.g. has the value 'no credits/ all paid'\n",
    "- The bottom line represents the correlations between all features and the target variable: account_status and loan_duration seem to be the most correlated values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b9e29",
   "metadata": {},
   "source": [
    "#### 8.1 Correlation loan_amount, loan_duration and target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d072bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(\n",
    "    data=credit_data,\n",
    "    x='loan_duration',\n",
    "    y='loan_amount',\n",
    "    hue='target',\n",
    "    palette={'good': 'skyblue', 'bad': 'coral'},\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.xlabel('Loan Duration')\n",
    "plt.ylabel('Loan Amount')\n",
    "plt.title('Loan Amount vs. Loan Duration Colored by Target')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100c94f",
   "metadata": {},
   "source": [
    "Conclusion: Short and low loans are usually granted. Short and high loans have the worst chance of acceptance. In general: the longer the loan, the higher it usually is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba77ed2",
   "metadata": {},
   "source": [
    "### 8.2 Correlation loan_amount, purpose/ age and target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f856d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(211)\n",
    "g1 = sns.boxplot(x=\"loan_purpose\", y=\"loan_amount\", data=credit_data, palette=\"husl\", hue=\"target\")\n",
    "g1.set_xlabel(\"Loan Purpose\", fontsize=12)\n",
    "g1.set_ylabel(\"Loan Amount\", fontsize=12)\n",
    "g1.set_title(\"Loan Amount Distribution by Loan Purpose\", fontsize=16)\n",
    "\n",
    "bins = [0, 20, 30, 40, 50, np.inf]\n",
    "labels = ['<20', '20-29', '30-39', '40-49', '50+']\n",
    "credit_data['age_bucket'] = pd.cut(credit_data['applicant_age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "plt.subplot(212)\n",
    "g2 = sns.boxplot(x=\"age_bucket\", y=\"loan_amount\", data=credit_data, palette=\"husl\", hue=\"target\")\n",
    "g2.set_xlabel(\"Applicant Age Bucket\", fontsize=12)\n",
    "g2.set_ylabel(\"Loan Amount\", fontsize=12)\n",
    "g2.set_title(\"Loan Amount Distribution by Applicant Age\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f30c88",
   "metadata": {},
   "source": [
    "Conclusions on purposes:\n",
    "- Usually smaller loans for used cars are accepted more easily\n",
    "- General tendency to accepting smaller loans (less risk for the bank) - except for repairs\n",
    "\n",
    "Conclusions on ages:\n",
    "- Pople above 50 are less trusted with large loans\n",
    "- People between 30 and 40 seem most trustworthy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998bbaa",
   "metadata": {},
   "source": [
    "### 9. Applying Random Forest Classifier to Assess Feature Importance\n",
    "\n",
    "Next to the feature correlation matrix, we are now using a simple Random Forest Classifier as a second source on which features might be important to use for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b590ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare data for Random Forest model\n",
    "X = pd.get_dummies(credit_data.drop(target_col, axis=1), drop_first=True)\n",
    "y = credit_data[target_col]\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Aggregate importances for original features\n",
    "agg_importance = {}\n",
    "for col in credit_data.drop(target_col, axis=1).columns:\n",
    "    # Find all dummy columns that start with this feature name + '_'\n",
    "    related_features = [f for f in feature_names if f == col or f.startswith(col + '_')]\n",
    "    agg_importance[col] = sum(importances[list(feature_names).index(f)] for f in related_features)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "agg_importance_df = pd.DataFrame({'feature': list(agg_importance.keys()), 'importance': list(agg_importance.values())})\n",
    "agg_importance_df = agg_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.barplot(x='importance', y='feature', data=agg_importance_df)\n",
    "plt.title('Aggregated Feature Importance from Random Forest')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Original Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454bffcc",
   "metadata": {},
   "source": [
    "Conclusions: In combination with the results from earlier correlation matrices it seems promising to use combination of loan_amount, account_status, loan_duration, applicant_age and loan_purpose, among other factors, as input features for the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf2269",
   "metadata": {},
   "source": [
    "### 10. Summary of Potential Data Flaws\n",
    "\n",
    "- **Class Imbalance:** There is a moderate imbalance in the target variable, with more \"good\" than \"bad\" outcomes. This can bias models toward the majority class.\n",
    "- **Sparse Categories:** Some categorical values are rare, which can lead to unreliable estimates for those groups.\n",
    "\n",
    "Papers like ['Using neural network rule extraction and decision tables for credit-risk evaluation' by Bart Baesens et. al.](https://www.jstor.org/stable/4133928) discuss the challenges of class imbalance and categorical encoding in very similar credit risk datasets, confirming that these are recognized issues in the literature.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
